{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted accuracy -  0.9980687439963793\n",
      "Mean Absolute Error: 767.9799372480995\n",
      "Mean Squared Error: 35565200.13532048\n",
      "Root Mean Squared Error: 5963.656607763434\n"
     ]
    }
   ],
   "source": [
    "# Analysis of Real Estate Sales from 2001 to 2016\n",
    "#\n",
    "# Notebook focus: Finding factors driving differences in Actual Sales' Values vs. Assessed Sales' Value; \n",
    "# Using Random Forest and a Decision Tree from the SKLearn Library to derive numerical results. \n",
    "#\n",
    "# Author: Jane Nikolova\n",
    "# Occupation: Senior Consultant\n",
    "# All Rights Reserved. \n",
    "# Date: May, 2019\n",
    "#\n",
    "# Data Source - Real Estate Sales 2001-2016\n",
    "# https://catalog.data.gov/dataset/real-estate-sales-2001-2016\n",
    "\n",
    "# Libraries - SKLearn, Pandas, NumPy, Matplotlib \n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image\n",
    "from matplotlib.pyplot import *\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.externals.six import StringIO \n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image  \n",
    "\n",
    "# LOAD AND PRE-PROCESS THE DATA - \n",
    "\n",
    "# Use data loading class - 'DataFactory' - to load, transform the data on a basic level, \n",
    "# and engineer some new features out of 'sale date', 'list date' and remarks - \n",
    "\n",
    "import DataFactory\n",
    "dataFile = 'Real_Estate_Sales_2001-2016.csv'\n",
    "XY = DataFactory.allDataForOutcome(dataFile, 'SalesRatio')\n",
    "X = XY[0] # All independent variables (predictors)\n",
    "Y = XY[1] # Sales-Ratio - Variable of interest (predicted variable)\n",
    "\n",
    "#print(X.shape)\n",
    "#print(Y.shape)\n",
    "\n",
    "# ML PREDICTION MODELS - \n",
    "# All models and menthods are from SKLearn -  \n",
    "\n",
    "# DECISION TREE - Regressor - \n",
    "# The problem assesses the impact for a continuous variable. In the case of \n",
    "# categorical outcome, we would use the Classifier version of the Decision Tree. \n",
    "\n",
    "#Create train & validation sets - 30% validation, 70% train sets -\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)  \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "regressor = DecisionTreeRegressor()  \n",
    "regressor.fit(X_train, y_train) \n",
    "\n",
    "#Predict - \n",
    "y_pred = regressor.predict(X_test) \n",
    "\n",
    "#Accuracy for predicted vs. actual - \n",
    "from sklearn.metrics import r2_score\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(\"predicted accuracy - \", score)\n",
    "#predicted accuracy -  0.9979612350160512 \n",
    "\n",
    "#Evaluate the performance of the algorithm -  MAE, MSE, RMSE - \n",
    "from sklearn import metrics  \n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) \n",
    "\n",
    "# Mean Absolute Error: 771.5091917079061\n",
    "# Mean Squared Error: 38371152.832423516\n",
    "# Root Mean Squared Error: 6194.44532080343\n",
    "\n",
    "# Using cross-validation find the optimal tree depth - \n",
    "from sklearn.model_selection import cross_val_score\n",
    "all_scores = []\n",
    "best_score = -1\n",
    "best_depth = 0\n",
    "# Check for trees with depth from 1 to 10\n",
    "for i in range(1,9):\n",
    "    treereg = DecisionTreeRegressor(max_depth=i, random_state=1)\n",
    "    scores = cross_val_score(treereg, X_train, y_train, cv=3, scoring='neg_mean_squared_error')\n",
    "    current_score = np.mean(np.sqrt(-scores))\n",
    "    if current_score < best_score or best_score == -1:\n",
    "        best_score = current_score\n",
    "        best_depth = i\n",
    "    all_scores.append(current_score)\n",
    "    \n",
    "print(\"Best score:\", best_score)# 35643.33404465698\n",
    "print(\"Best depth:\", best_depth)# 8! \n",
    "plt.plot(range(1, 9), all_scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('x=max tree depth')\n",
    "plt.show()\n",
    "#Create a tree with the best depth and plot the most important features - \n",
    "    \n",
    "#Visualize the tree, plot the features importance, other model characteristics - \n",
    "\n",
    "# ENSEMBLE METHODS - \n",
    "\n",
    "# Bagging methods' group - minimizing bias (variance - bias trade-off) -\n",
    "# AdaBoost - \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Random Forest -\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Boosting methods' group - minimizing bias (variance - bias trade-off) - \n",
    "# Gradient Booster - \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Neural Networks - relevance?\n",
    "\n",
    "# ANY OTHER - MLR (Multiple Linear Regression + Stepwise), k-NN (k-Nearest Neighbor)\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "\n",
    "# MODEL SELECTION - \n",
    "\n",
    "# UNSUPERVISED LEARNING -\n",
    "# Find clusters - k-Means, and exploring the methods provided in SKLearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
